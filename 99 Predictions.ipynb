{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T11:18:42.068656Z",
     "start_time": "2020-05-02T11:18:40.854995Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T11:18:43.150906Z",
     "start_time": "2020-05-02T11:18:42.070654Z"
    }
   },
   "outputs": [],
   "source": [
    "countVectorizer = pickle.load(open('BinaryCountVectorizer.pkl', 'rb'))\n",
    "tfidfVectorizer = pickle.load(open('TfidfVectorizer.pkl', 'rb'))\n",
    "\n",
    "tfidfModel = pickle.load(open('TFIDFVotingClassifier.pkl', 'rb'))\n",
    "featuresModel = pickle.load(open('CustomFeaturesVotingClassifier.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T11:18:43.776281Z",
     "start_time": "2020-05-02T11:18:43.153929Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Ritvik\\Anaconda3\\envs\\datascience\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from scipy.sparse import hstack\n",
    "from scipy.spatial.distance import hamming, cosine\n",
    "\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    \n",
    "    x = re.sub(r'\\W', ' ', str(x))\n",
    "    \n",
    "    x = re.sub(r\"\\s+\", ' ', str(x))\n",
    "    \n",
    "    x = ' '.join(ps.stem(word) for word in x.split())\n",
    "    \n",
    "    soup = BeautifulSoup(str(x))\n",
    "    x = soup.get_text()\n",
    "\n",
    "    x = ' '.join([word for word in x.split() if word not in STOP_WORDS])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def lcs(X , Y): \n",
    "    m = len(X) \n",
    "    n = len(Y) \n",
    "\n",
    "    L = [[None]*(n+1) for i in range(m+1)] \n",
    "  \n",
    "    for i in range(m+1): \n",
    "        for j in range(n+1): \n",
    "            if i == 0 or j == 0 : \n",
    "                L[i][j] = 0\n",
    "            elif X[i-1] == Y[j-1]: \n",
    "                L[i][j] = L[i-1][j-1]+1\n",
    "            else: \n",
    "                L[i][j] = max(L[i-1][j] , L[i][j-1]) \n",
    "  \n",
    "    return L[m][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T11:18:43.817204Z",
     "start_time": "2020-05-02T11:18:43.780242Z"
    }
   },
   "outputs": [],
   "source": [
    "def generateFeatures(text1, text2):\n",
    " \n",
    "    abs_token_diff = abs(len(text1.split())-len(text2.split()))\n",
    "    avg_num_token = (len(text1.split())+len(text2.split()))/2\n",
    "    rel_token_diff = abs_token_diff/avg_num_token\n",
    "    token_intersection = len(set(text1.split())&set(text2.split()))\n",
    "    token_union = len(set(text1.split())|set(text2.split()))\n",
    "    jaccard_similarity_token = token_intersection/token_union\n",
    "    lcs_token = lcs(text1.split(), text2.split())\n",
    "    lcs_token_ratio = lcs_token/avg_num_token\n",
    "    \n",
    "    text1_clean = preprocess(text1)\n",
    "    text2_clean = preprocess(text2)\n",
    "    \n",
    "    abs_word_diff = abs(len(text1_clean.split())-len(text2_clean.split()))\n",
    "    avg_num_word = (len(text1_clean.split())+len(text2_clean.split()))/2\n",
    "    rel_word_diff = abs_word_diff/avg_num_word\n",
    "    word_intersection = len(set(text1_clean.split())&set(text2_clean.split()))\n",
    "    word_union = len(set(text1_clean.split())|set(text2_clean.split()))\n",
    "    jaccard_similarity_word = word_intersection/word_union\n",
    "    lcs_word = lcs(text1_clean.split(), text2_clean.split())\n",
    "    lcs_word_ratio = lcs_word/avg_num_word\n",
    "    \n",
    "    fuzz_simple_ratio = fuzz.ratio(str(text1_clean).split(), str(text2_clean).split())\n",
    "    fuzz_partial_ratio = fuzz.partial_ratio(str(text1_clean).split(), str(text2_clean).split())\n",
    "    fuzz_token_sort_ratio = fuzz.token_sort_ratio(str(text1_clean).split(), str(text2_clean).split())\n",
    "    fuzz_token_set_ratio = fuzz.token_set_ratio(str(text1_clean).split(), str(text2_clean).split())\n",
    "    \n",
    "    v1 = countVectorizer.transform([text1_clean])\n",
    "    v2 = countVectorizer.transform([text2_clean])\n",
    "    \n",
    "    hamming_distance = hamming(v1.toarray()[0], v2.toarray()[0])\n",
    "    \n",
    "    v1 = tfidfVectorizer.transform([text1_clean])\n",
    "    v2 = tfidfVectorizer.transform([text2_clean])\n",
    "    \n",
    "    cosine_distance = cosine(v1.toarray()[0], v2.toarray()[0])\n",
    "    \n",
    "    features = {\n",
    "        'abs_token_diff': abs_token_diff, 'avg_num_token': avg_num_token, 'rel_token_diff': rel_token_diff,\n",
    "        'token_intersection': token_intersection, 'token_union': token_union, \n",
    "        'jaccard_similarity_token': jaccard_similarity_token, 'lcs_token': lcs_token, 'lcs_token_ratio': lcs_token_ratio,\n",
    "        'abs_word_diff': abs_word_diff, 'avg_num_word': avg_num_word, 'rel_word_diff': rel_word_diff,\n",
    "        'word_intersection': word_intersection, 'word_union': word_union, \n",
    "        'jaccard_similarity_word': jaccard_similarity_word, 'lcs_word': lcs_word, 'lcs_word_ratio': lcs_word_ratio,\n",
    "        'fuzz_simple_ratio': fuzz_simple_ratio, 'fuzz_partial_ratio': fuzz_partial_ratio, \n",
    "        'fuzz_token_sort_ratio': fuzz_token_sort_ratio, 'fuzz_token_set_ratio': fuzz_token_set_ratio,\n",
    "        'hamming_distance': hamming_distance, 'cosine_distance': cosine_distance\n",
    "    }\n",
    "    \n",
    "    features = pd.DataFrame(features, index=[0])\n",
    "    \n",
    "    vects = hstack([v1, v2])\n",
    "    \n",
    "    return features, vects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T11:18:43.953131Z",
     "start_time": "2020-05-02T11:18:43.818140Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPredictions(feats, vects):\n",
    "    labels = {\n",
    "        0: 'unique', 1: 'duplicate'\n",
    "    }\n",
    "    \n",
    "    probabilities = pd.DataFrame(np.concatenate([\n",
    "        tfidfModel.estimators_[0].predict_proba(vects),\n",
    "        tfidfModel.estimators_[1].predict_proba(vects),\n",
    "        tfidfModel.estimators_[2].predict_proba(vects),\n",
    "        tfidfModel.estimators_[3].predict_proba(vects),\n",
    "        featuresModel.estimators_[0].predict_proba(feats),\n",
    "        featuresModel.estimators_[1].predict_proba(feats),\n",
    "        featuresModel.estimators_[2].predict_proba(feats)\n",
    "    ], axis=0), index=['LR vects 1', 'LR vects 10', 'LR vects 100', 'SGD vects', 'LR', 'XGB', 'GB'], \n",
    "                                 columns=['unique', 'duplicate'])\n",
    "    \n",
    "    results = probabilities['duplicate'].apply(lambda x : labels[round(x)])\n",
    "    \n",
    "    return probabilities, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T11:18:44.096675Z",
     "start_time": "2020-05-02T11:18:43.957122Z"
    },
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "q1 = 'hello how are you?'\n",
    "q2 = 'who the hell are you?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T11:18:44.452977Z",
     "start_time": "2020-05-02T11:18:44.099667Z"
    },
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "feats, vects = generateFeatures(q1, q2)\n",
    "p, r = getPredictions(feats, vects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T11:18:44.541741Z",
     "start_time": "2020-05-02T11:18:44.458962Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique</th>\n",
       "      <th>duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LR vects 1</td>\n",
       "      <td>0.813215</td>\n",
       "      <td>0.186785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LR vects 10</td>\n",
       "      <td>0.751844</td>\n",
       "      <td>0.248156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LR vects 100</td>\n",
       "      <td>0.662149</td>\n",
       "      <td>0.337851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SGD vects</td>\n",
       "      <td>0.654381</td>\n",
       "      <td>0.345619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LR</td>\n",
       "      <td>0.164120</td>\n",
       "      <td>0.835880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>XGB</td>\n",
       "      <td>0.725316</td>\n",
       "      <td>0.274684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GB</td>\n",
       "      <td>0.742827</td>\n",
       "      <td>0.257173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                unique  duplicate\n",
       "LR vects 1    0.813215   0.186785\n",
       "LR vects 10   0.751844   0.248156\n",
       "LR vects 100  0.662149   0.337851\n",
       "SGD vects     0.654381   0.345619\n",
       "LR            0.164120   0.835880\n",
       "XGB           0.725316   0.274684\n",
       "GB            0.742827   0.257173"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T11:18:44.655954Z",
     "start_time": "2020-05-02T11:18:44.547727Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LR vects 1         unique\n",
       "LR vects 10        unique\n",
       "LR vects 100       unique\n",
       "SGD vects          unique\n",
       "LR              duplicate\n",
       "XGB                unique\n",
       "GB                 unique\n",
       "Name: duplicate, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
